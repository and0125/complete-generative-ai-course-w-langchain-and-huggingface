# Important components and Modules in Langchain

## Basic Components

Just went over the basic features and setup of langsmith.

If you want to make a PDF file as a document Q&A Application, if you are creating a Gen AI app. Then if a user asks any query, they should be able to send the query to a model, and then the model would access the PDF for a data source. This is the _Retrieval Augmented Generation_ setup.

Then if anyone is giving a question to this datasource, you can get an answer from the PDF.

For this application, we'll understand the step by step process to setup this application.

The common steps for this setup are:

- loading data from a data source
- splitting the data into smaller chunks; this is because LLMs have limitations on how much context they can accept; and this limits the amount of data from a data source we can use for a single queries.
- embedding: taking the text and converting it into vectors.
- store the data into a vector database so that the embeddings are retained for querying against.

For the full system setup, when a user then asks a question, the flow goes to the vector database to retrieve some information. The information retrieved is then placed into a larger prompt, along with the initial question. Then the larger prompt is passed to an LLM, and then the LLM gives the answer.

A _retrieval chain_ is an interface for querying information within a vector database for this retrieval part of the query.

For each of these steps, there are multiple techniques that you could load:

- different ways to load data
- different ways to split data
- to embed data
- to store data (FAISS< ChromaDB, AstraDB)
- to create a retrieval chain

These are the very important components of langchain

The retrieval chain is an interface that queries the Vector store database; and the response from the retrieval chain gives you the context that's picked up for the prompt template; the context and initial question are all passed to the LLM, and then given an answer.

step by step we will break down these common components of a Gen AI deployment with Langchain.

## Document Loaders

this is the first step for processing data into an LLM. _data ingestion_ or _document loaders_ are the way we read data in from documents.

see `document-loader.ipynb`.

Get in the habit of reading the documentation; but know that it's scattered. His videos order the way in which different objects are implemented for developing applications.

There is a way to load data from airtable, Cassandra, Dropbox, GitHub; etc. THere are so many different options for loading data; including pandas dataframes twitter, etc. Weather. WhatsApp. Trello.

Referenced the `attention is all you need` article as a document.

There are plenty of document loaders to be able to use with different data sets.

## Text Splitting Techniques

The most general text splitting technique is the recursive Character Text Splitter but there are other text splitter techniques like:

- text Character Text Splitter: splits text based on specific characters; defaults to `\n\n` but you can pick the specific character to split on.
- HTML header text splitter: splits out specific HTML tags from either an HTML document or directly from a url.
- Recursive JSON splitter: if you recieve a long json response, this json splitter chunks up the data depth first, and attempts to keep nested json objects whol but will split them if need be. If the value is not nested. IF you need a hard cap on the chunk size consider using this with a recursive text splitter also. great for an entire API response.

see `data-splitting` file for use.

For the JSON splitter, it's important to know if your API data is returned inside of a list `[...]` or inside the object brackets `{...}`. This seemingly small change is the difference between being a root-level list, or a root-level json object, respectively. The JSON splitter has a built in parameter to handle root-level lists called `convert_lists=`True`. But the best and most reliable way to do this is to wrap the list returned in a dictionary object, which is what was done in the data splitting example.

## Introduction to Open AI Embedding

We went over document loading, then data splitting or data transformation.

Then we'll take those chunks and convert the text chunk into vectors.

This session will focus on a few techniques to do this:

- Open AI
- Ollama
- HuggingFace

each of these companies have embedding techniques. There are plenty of other ways to do this, including from Google Gemini, Anthropic, etc. But these 3 would be sufficient for understanding how to do this.

OpenAI is paid; Ollama is open source; Huggingface is also open source and free.

**NOTE**: I'll likely use Anthropic for this instead of Open AI because I pay for that right now. Added a new API key for this.

Embedding techniques convert text into vectors.

### Anthropic Embedding

Its a bit different than the standard embedding shown in the tutorial.

1. Import the `voyageai` (`pip install voyageai`) package, and use its client object to embed the text.

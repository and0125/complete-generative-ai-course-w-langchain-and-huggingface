# Important components and Modules in Langchain

## Basic Components

Just went over the basic features and setup of langsmith.

If you want to make a PDF file as a document Q&A Application, if you are creating a Gen AI app. Then if a user asks any query, they should be able to send the query to a model, and then the model would access the PDF for a data source. This is the _Retrieval Augmented Generation_ setup.

Then if anyone is giving a question to this datasource, you can get an answer from the PDF.

For this application, we'll understand the step by step process to setup this application.

The common steps for this setup are:

- loading data from a data source
- splitting the data into smaller chunks; this is because LLMs have limitations on how much context they can accept; and this limits the amount of data from a data source we can use for a single queries.
- embedding: taking the text and converting it into vectors.
- store the data into a vector database so that the embeddings are retained for querying against.

For the full system setup, when a user then asks a question, the flow goes to the vector database to retrieve some information. The information retrieved is then placed into a larger prompt, along with the initial question. Then the larger prompt is passed to an LLM, and then the LLM gives the answer.

A _retrieval chain_ is an interface for querying information within a vector database for this retrieval part of the query.

For each of these steps, there are multiple techniques that you could load:

- different ways to load data
- different ways to split data
- to embed data
- to store data (FAISS< ChromaDB, AstraDB)
- to create a retrieval chain

These are the very important components of langchain

The retrieval chain is an interface that queries the Vector store database; and the response from the retrieval chain gives you the context that's picked up for the prompt template; the context and initial question are all passed to the LLM, and then given an answer.

step by step we will break down these common components of a Gen AI deployment with Langchain.

## Document Loaders

this is the first step for processing data into an LLM. _data ingestion_ or _document loaders_ are the way we read data in from documents.

see `document-loader.ipynb`.

Get in the habit of reading the documentation; but know that it's scattered. His videos order the way in which different objects are implemented for developing applications.

There is a way to load data from airtable, Cassandra, Dropbox, GitHub; etc. THere are so many different options for loading data; including pandas dataframes twitter, etc. Weather. WhatsApp. Trello.

Referenced the `attention is all you need` article as a document.
